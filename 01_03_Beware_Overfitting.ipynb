{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"03 - Beware Overfitting.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOQlyARpN+uAx1HSEhWquYt"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"TW66bDnkEN3r","colab_type":"text"},"source":["# 03 - Beware Overfitting"]},{"cell_type":"markdown","metadata":{"id":"Xc6rAyhzEs09","colab_type":"text"},"source":["### **Overfitting**<br> \n","When your model captures patterns in your training data too well - meaning it doesn't generalize well to unseen data."]},{"cell_type":"markdown","metadata":{"id":"H0cS5zhSLa8T","colab_type":"text"},"source":["## **Preventing Overfitting** \n","\n","**Regularization:** Introducing a penalty for overly complex features that reduces - or eliminates - their weight in our model.\n","\n","Two common types of regularization include **Lasso or L1 regularization** and **Ridge or L2 regularization**."]},{"cell_type":"code","metadata":{"id":"TGFIGXA8O9cT","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595436146104,"user_tz":420,"elapsed":283,"user":{"displayName":"Brett Vanderblock","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhmgXUM06I_0Utz7PNi8ns4XuV5Uba2Tr63jSQX=s64","userId":"08986418591522242227"}}},"source":["import pandas as pd"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"uG9lox90PEqJ","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595436231998,"user_tz":420,"elapsed":269,"user":{"displayName":"Brett Vanderblock","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhmgXUM06I_0Utz7PNi8ns4XuV5Uba2Tr63jSQX=s64","userId":"08986418591522242227"}}},"source":["from sklearn.datasets import load_breast_cancer\n","df = load_breast_cancer()\n","bc = pd.DataFrame(df.data,columns=df.feature_names)\n","bc['target'] = pd.Series(df.target)"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"VVoWTtWNPF05","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":258},"executionInfo":{"status":"ok","timestamp":1595436237879,"user_tz":420,"elapsed":335,"user":{"displayName":"Brett Vanderblock","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhmgXUM06I_0Utz7PNi8ns4XuV5Uba2Tr63jSQX=s64","userId":"08986418591522242227"}},"outputId":"aa6989ad-69c6-4f2f-ac1a-a96fc53fbf6d"},"source":["bc.head()"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>mean radius</th>\n","      <th>mean texture</th>\n","      <th>mean perimeter</th>\n","      <th>mean area</th>\n","      <th>mean smoothness</th>\n","      <th>mean compactness</th>\n","      <th>mean concavity</th>\n","      <th>mean concave points</th>\n","      <th>mean symmetry</th>\n","      <th>mean fractal dimension</th>\n","      <th>radius error</th>\n","      <th>texture error</th>\n","      <th>perimeter error</th>\n","      <th>area error</th>\n","      <th>smoothness error</th>\n","      <th>compactness error</th>\n","      <th>concavity error</th>\n","      <th>concave points error</th>\n","      <th>symmetry error</th>\n","      <th>fractal dimension error</th>\n","      <th>worst radius</th>\n","      <th>worst texture</th>\n","      <th>worst perimeter</th>\n","      <th>worst area</th>\n","      <th>worst smoothness</th>\n","      <th>worst compactness</th>\n","      <th>worst concavity</th>\n","      <th>worst concave points</th>\n","      <th>worst symmetry</th>\n","      <th>worst fractal dimension</th>\n","      <th>target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>17.99</td>\n","      <td>10.38</td>\n","      <td>122.80</td>\n","      <td>1001.0</td>\n","      <td>0.11840</td>\n","      <td>0.27760</td>\n","      <td>0.3001</td>\n","      <td>0.14710</td>\n","      <td>0.2419</td>\n","      <td>0.07871</td>\n","      <td>1.0950</td>\n","      <td>0.9053</td>\n","      <td>8.589</td>\n","      <td>153.40</td>\n","      <td>0.006399</td>\n","      <td>0.04904</td>\n","      <td>0.05373</td>\n","      <td>0.01587</td>\n","      <td>0.03003</td>\n","      <td>0.006193</td>\n","      <td>25.38</td>\n","      <td>17.33</td>\n","      <td>184.60</td>\n","      <td>2019.0</td>\n","      <td>0.1622</td>\n","      <td>0.6656</td>\n","      <td>0.7119</td>\n","      <td>0.2654</td>\n","      <td>0.4601</td>\n","      <td>0.11890</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>20.57</td>\n","      <td>17.77</td>\n","      <td>132.90</td>\n","      <td>1326.0</td>\n","      <td>0.08474</td>\n","      <td>0.07864</td>\n","      <td>0.0869</td>\n","      <td>0.07017</td>\n","      <td>0.1812</td>\n","      <td>0.05667</td>\n","      <td>0.5435</td>\n","      <td>0.7339</td>\n","      <td>3.398</td>\n","      <td>74.08</td>\n","      <td>0.005225</td>\n","      <td>0.01308</td>\n","      <td>0.01860</td>\n","      <td>0.01340</td>\n","      <td>0.01389</td>\n","      <td>0.003532</td>\n","      <td>24.99</td>\n","      <td>23.41</td>\n","      <td>158.80</td>\n","      <td>1956.0</td>\n","      <td>0.1238</td>\n","      <td>0.1866</td>\n","      <td>0.2416</td>\n","      <td>0.1860</td>\n","      <td>0.2750</td>\n","      <td>0.08902</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>19.69</td>\n","      <td>21.25</td>\n","      <td>130.00</td>\n","      <td>1203.0</td>\n","      <td>0.10960</td>\n","      <td>0.15990</td>\n","      <td>0.1974</td>\n","      <td>0.12790</td>\n","      <td>0.2069</td>\n","      <td>0.05999</td>\n","      <td>0.7456</td>\n","      <td>0.7869</td>\n","      <td>4.585</td>\n","      <td>94.03</td>\n","      <td>0.006150</td>\n","      <td>0.04006</td>\n","      <td>0.03832</td>\n","      <td>0.02058</td>\n","      <td>0.02250</td>\n","      <td>0.004571</td>\n","      <td>23.57</td>\n","      <td>25.53</td>\n","      <td>152.50</td>\n","      <td>1709.0</td>\n","      <td>0.1444</td>\n","      <td>0.4245</td>\n","      <td>0.4504</td>\n","      <td>0.2430</td>\n","      <td>0.3613</td>\n","      <td>0.08758</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>11.42</td>\n","      <td>20.38</td>\n","      <td>77.58</td>\n","      <td>386.1</td>\n","      <td>0.14250</td>\n","      <td>0.28390</td>\n","      <td>0.2414</td>\n","      <td>0.10520</td>\n","      <td>0.2597</td>\n","      <td>0.09744</td>\n","      <td>0.4956</td>\n","      <td>1.1560</td>\n","      <td>3.445</td>\n","      <td>27.23</td>\n","      <td>0.009110</td>\n","      <td>0.07458</td>\n","      <td>0.05661</td>\n","      <td>0.01867</td>\n","      <td>0.05963</td>\n","      <td>0.009208</td>\n","      <td>14.91</td>\n","      <td>26.50</td>\n","      <td>98.87</td>\n","      <td>567.7</td>\n","      <td>0.2098</td>\n","      <td>0.8663</td>\n","      <td>0.6869</td>\n","      <td>0.2575</td>\n","      <td>0.6638</td>\n","      <td>0.17300</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>20.29</td>\n","      <td>14.34</td>\n","      <td>135.10</td>\n","      <td>1297.0</td>\n","      <td>0.10030</td>\n","      <td>0.13280</td>\n","      <td>0.1980</td>\n","      <td>0.10430</td>\n","      <td>0.1809</td>\n","      <td>0.05883</td>\n","      <td>0.7572</td>\n","      <td>0.7813</td>\n","      <td>5.438</td>\n","      <td>94.44</td>\n","      <td>0.011490</td>\n","      <td>0.02461</td>\n","      <td>0.05688</td>\n","      <td>0.01885</td>\n","      <td>0.01756</td>\n","      <td>0.005115</td>\n","      <td>22.54</td>\n","      <td>16.67</td>\n","      <td>152.20</td>\n","      <td>1575.0</td>\n","      <td>0.1374</td>\n","      <td>0.2050</td>\n","      <td>0.4000</td>\n","      <td>0.1625</td>\n","      <td>0.2364</td>\n","      <td>0.07678</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   mean radius  mean texture  ...  worst fractal dimension  target\n","0        17.99         10.38  ...                  0.11890       0\n","1        20.57         17.77  ...                  0.08902       0\n","2        19.69         21.25  ...                  0.08758       0\n","3        11.42         20.38  ...                  0.17300       0\n","4        20.29         14.34  ...                  0.07678       0\n","\n","[5 rows x 31 columns]"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"CisOTwU7RD3I","colab_type":"text"},"source":["**Example: L1 regularization in a Logistic Regression model.**"]},{"cell_type":"markdown","metadata":{"id":"_YcLvey5sLAt","colab_type":"text"},"source":["Train/Test Split"]},{"cell_type":"code","metadata":{"id":"svB_x_8WO8Pu","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595436374771,"user_tz":420,"elapsed":269,"user":{"displayName":"Brett Vanderblock","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhmgXUM06I_0Utz7PNi8ns4XuV5Uba2Tr63jSQX=s64","userId":"08986418591522242227"}}},"source":["from sklearn.model_selection import train_test_split\n","\n","X, y = bc.iloc[:, :-1].values, bc.iloc[:, -1].values\n","\n","X_train, X_test, y_train, y_test =\\\n","    train_test_split(X, y, \n","                     test_size=0.3, \n","                     random_state=0, \n","                     stratify=y)"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TwkPlmTGsO8u","colab_type":"text"},"source":["Scale features"]},{"cell_type":"code","metadata":{"id":"VrNqxvERPj3J","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595436514174,"user_tz":420,"elapsed":288,"user":{"displayName":"Brett Vanderblock","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhmgXUM06I_0Utz7PNi8ns4XuV5Uba2Tr63jSQX=s64","userId":"08986418591522242227"}}},"source":["from sklearn.preprocessing import StandardScaler\n","\n","stdsc = StandardScaler()\n","X_train_std = stdsc.fit_transform(X_train)\n","X_test_std = stdsc.transform(X_test)"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZKoeOhE3Pu5d","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1595436819465,"user_tz":420,"elapsed":308,"user":{"displayName":"Brett Vanderblock","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhmgXUM06I_0Utz7PNi8ns4XuV5Uba2Tr63jSQX=s64","userId":"08986418591522242227"}},"outputId":"5e75c010-e046-402b-f28d-febbf1a0095b"},"source":["from sklearn.linear_model import LogisticRegression\n","\n","\n","lr = LogisticRegression(penalty='l1', C=0.2, solver='liblinear', multi_class='ovr')\n","lr.fit(X_train_std, y_train)\n","\n","print('Training accuracy:', round(lr.score(X_train_std, y_train),3))\n","print('Test accuracy:', round(lr.score(X_test_std, y_test),3))"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Training accuracy: 0.99\n","Test accuracy: 0.947\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1gRaWb5nPwMf","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}